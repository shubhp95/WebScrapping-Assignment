{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "028d539d-c91a-4985-aa56-9320ee102a18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b2cd5c6-40fb-457a-9a21-9e608d01d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans1:Web scraping is the automated process of extracting data from websites. This involves fetching web pages and parsing the content to collect and store information in a structured format\n",
    "\n",
    "#Uses-\n",
    "#a.Data Collection\n",
    "#b.Competitive Analysis\n",
    "#c.Market Research\n",
    "#d.Content Aggregation\n",
    "\n",
    "#Three areas where it is used are-\n",
    "#1.News and Media:\n",
    "#2.E-commerce and Pricing Monitoring:\n",
    "#3.Real Estate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab429e8-ffcc-47cd-b9ad-7d31ef0a5779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans2:\n",
    "#a.HTTP Requests: Directly send HTTP requests to retrieve the HTML content of a web page.\n",
    "#b. HTML Parsing: Parse the HTML content to extract specific data using libraries that provide an interface to navigate and search HTML structures.\n",
    "#c.Web Scraping Frameworks: Use specialized frameworks designed to handle web scraping tasks, including automatic handling of requests, parsing, and data extraction.\n",
    "#d.Browser Automation: Use browser automation tools to interact with web pages, especially useful for websites that rely heavily on JavaScript.\n",
    "#e.APIs:Use provided APIs (if available) to get data directly in a structured format, bypassing the need to scrape HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb52d79-ba81-4e8e-9832-5d0c4b75ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans3:Beautiful Soup is a Python library used for parsing HTML and XML documents. It provides tools to navigate, search, and modify the parse tree in an easy-to-use way, making it a popular choice for web scraping tasks.\n",
    "#Uses_\n",
    "#a.Parsing HTML and XML\n",
    "#b.Simplified Navigation\n",
    "#c.Handling Malformed HTML\n",
    "#d.Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a96bf3-48af-484c-a451-33c287bbb33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans4:Uses\n",
    "#a.Creating a Web Interface\n",
    "#b.Serving Scraped Data\n",
    "#c.Scheduling and Automation\n",
    "#d.API Integration\n",
    "#e.Displaying Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97064fa0-69a8-4bae-abe6-c3baa53cb581",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans5:\n",
    "#1.Amazon EC2 (Elastic Compute Cloud):Provides scalable virtual servers to run web scraping scripts and applications. \n",
    "#2.Amazon RDS (Relational Database Service):Manages and scales relational databases to store structured data from web scraping.\n",
    "#3.Amazon API Gateway: Creates, deploys, and manages APIs for web scraping applications.\n",
    "#4.Amazon SQS (Simple Queue Service): Manages message queues for decoupling and scaling web scraping tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d79803-5c50-4b0b-923f-89f0e9735319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
